{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67de4a35-c08c-4dae-b74c-f8c5ca4b6733",
   "metadata": {},
   "source": [
    "# Distance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768b528c-c06c-4d1a-ba6a-1ca94d500eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "from   ast import literal_eval\n",
    "from   collections import Counter, defaultdict\n",
    "from   geopy import distance\n",
    "from   itertools import pairwise\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "from   unidecode import unidecode\n",
    "\n",
    "data_dir    = os.path.join('..', 'data')\n",
    "derived_dir = os.path.join(data_dir, 'derived')\n",
    "raw_dir     = os.path.join(data_dir, 'booknlp')\n",
    "geo_dir     = os.path.join(data_dir, 'geo')\n",
    "\n",
    "conlit_input_file = 'conlit.csv.gz'\n",
    "conlit_distances_output_file = 'conlit_distances.csv.gz'\n",
    "early_distances_output_file = 'early_distances.csv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654d317-01c5-47ba-b4e4-5cf4399f0e12",
   "metadata": {},
   "source": [
    "## Load derived data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c137454e-a1a9-4b29-b3a0-9817fab9651d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def string_to_list(x):\n",
    "    lst = literal_eval(x.replace(', nan', \"', ZZZZ'\").replace('[nan, ', \"'['\").replace(', nan]', \"']'\"))\n",
    "    return [i for i in lst if i != 'ZZZZ']\n",
    "\n",
    "conlit = pd.read_csv(\n",
    "    os.path.join(derived_dir, conlit_input_file), \n",
    "    index_col='book_id',\n",
    "    converters={\n",
    "        'gpe_places': string_to_list,\n",
    "        'nongpe_places': string_to_list,\n",
    "        'all_places': string_to_list,\n",
    "        'gpe_sequences': string_to_list\n",
    "    },\n",
    ")\n",
    "conlit = conlit.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2009fc1e-4663-4edc-bf48-c3202138f4d3",
   "metadata": {},
   "source": [
    "## CONLIT GPE distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e769576-ae28-4753-960a-bb9942c15ace",
   "metadata": {},
   "source": [
    "### Geo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "109e2c90-4087-41a0-8df4-ca6de344aa9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# regularize data to existing geo format\n",
    "punctuation_to_space = str.maketrans({key:' ' for key in string.punctuation})\n",
    "\n",
    "def regularize_string(place_string):\n",
    "    return(unidecode(' '.join(place_string.translate(punctuation_to_space).lower().split())))\n",
    "\n",
    "geo = pd.read_csv(\n",
    "    os.path.join(geo_dir, 'geo.tsv.gz'),\n",
    "    sep='\\t',\n",
    "    low_memory=False,\n",
    ")\n",
    "geo = geo.loc[geo.lang=='en']\n",
    "geo.set_index('text_string', inplace=True)\n",
    "\n",
    "# hand review data\n",
    "hand = pd.read_csv(\n",
    "    os.path.join(geo_dir, 'handreview.tsv'),\n",
    "    sep='\\t',\n",
    "    index_col='text_string'\n",
    ")\n",
    "\n",
    "# restore some items from C19 hand review\n",
    "hand.loc[\n",
    "    [\n",
    "        'hollywood', \n",
    "        'dallas', \n",
    "        'florence', \n",
    "        'kingston',\n",
    "        'berkeley', \n",
    "        'queens', \n",
    "        'phoenix', \n",
    "        'woodstock', \n",
    "        'surrey',\n",
    "        'orlando'\n",
    "    ], \n",
    "    'ignore'\n",
    "] = 0\n",
    "\n",
    "# improve alises\n",
    "hand.loc['kingston', 'alias_to'] = 'kingston jamaica'\n",
    "hand.loc['baltic', ['ignore', 'alias_to']] = [0, 'baltic sea']\n",
    "\n",
    "# alias places\n",
    "for original_place, alias_to in hand.loc[(~hand.alias_to.isna()) & (~hand.ignore.equals(1)) & (hand.alias_to.isin(geo.index)), 'alias_to'].items():\n",
    "    geo.loc[original_place] = geo.loc[alias_to]\n",
    "    \n",
    "# drop ignored places\n",
    "geo.drop(hand.loc[hand.ignore==1].index, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5abb976a-9cb8-4be3-a643-d36b84ff23c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop unused places\n",
    "used_gpes = Counter()\n",
    "for sequence in conlit.gpe_sequences:\n",
    "    used_gpes.update([regularize_string(i) for i in sequence])\n",
    "geo.drop(geo.loc[~geo.index.isin(used_gpes)].index, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967cdeed-45eb-48e7-8a1b-f73de5835064",
   "metadata": {},
   "source": [
    "### Sequence distances\n",
    "\n",
    "For each volume sequence, look up each place, get lat/lon, calculate distance from previous place, sum over sequential path.\n",
    "\n",
    "Performs location aliasing, ignore known-bad places, and zero out sequence steps that move between admin levels within the same admin entity (e.g., `Boston -> United States` or `UK -> England`). The last step has room for improvement: we don't deal with `admin_2` level and below (the distances involved are small), nor with continents (source geo data doesn't place countries in continents, plus, I think there's a meaningful sense in which there's distance between, e.g., `Paris` and `Europe` in a way that there isn't between `Boston` and `USA`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54ace81b-4551-44ec-bebe-92ba10b3d92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lat_lon(place_string, geo_data, extended=False):\n",
    "    '''Assumes lookup string has been regularized if necessary'''\n",
    "    try:\n",
    "        if extended:\n",
    "            return(geo_data.loc[place_string, ['lat', 'lon', 'country_short', 'admin_1_std', 'location_type']])\n",
    "        else:\n",
    "            return(geo_data.loc[place_string, ['lat', 'lon']])\n",
    "    except (ValueError, KeyError):\n",
    "        return(None)\n",
    "\n",
    "def hop_distance(location1, location2, geo_data, return_zero_dist=False):\n",
    "    '''\n",
    "    Takes two location strings, returns fancy distance between them in miles.\n",
    "    return_zero_dist: if True, return zero distances where we would otherwise return None\n",
    "    '''\n",
    "    dist = None\n",
    "    if location1 in distances and location2 in distances[location1]:\n",
    "        return distances[location1][location2]\n",
    "    else:\n",
    "        loc1 = get_lat_lon(location1, geo_data, extended=True)\n",
    "        loc2 = get_lat_lon(location2, geo_data, extended=True)\n",
    "        if loc1 is None or loc2 is None: pass # should never happen, but check\n",
    "        elif loc1.equals(loc2): pass # ignore identical places, even if called different names\n",
    "        # eliminate place -> higher-order place in same admin area\n",
    "        elif (loc1.location_type=='country' or loc2.location_type=='country') and \\\n",
    "           (loc2.country_short==loc1.country_short): pass\n",
    "        elif (loc1.location_type=='administrative_area_level_1' or \\\n",
    "              loc2.location_type=='administrative_area_level_1') and \\\n",
    "             (loc2.country_short==loc1.country_short and \\\n",
    "              loc2.admin_1_std==loc1.admin_1_std): pass\n",
    "        else:\n",
    "            loc1 = loc1[['lat', 'lon']]\n",
    "            loc2 = loc2[['lat', 'lon']]   \n",
    "            dist = distance.distance(loc1, loc2).miles\n",
    "            distances[location1][location2] = dist\n",
    "            distances[location2][location1] = dist\n",
    "    if return_zero_dist and dist is None:\n",
    "        dist = 0.0\n",
    "    return(dist)\n",
    "\n",
    "def sequence_distance(sequence, geo_data, return_zero_dist=True):\n",
    "    # set data, regularize strings, and remove unknown locations\n",
    "    seq = [regularize_string(i) for i in sequence if regularize_string(i) in geo_data.index]\n",
    "    total_distance = 0.0\n",
    "    hop_distances = []\n",
    "    # calculate distance over pairwise hops\n",
    "    for location1, location2 in pairwise(seq):\n",
    "        dist = hop_distance(location1, location2, geo_data)\n",
    "        if dist != None:\n",
    "            total_distance += dist\n",
    "            hop_distances.append(dist)\n",
    "    # calculate start-finish distance\n",
    "    if return_zero_dist:\n",
    "        start_finish_miles = 0.0\n",
    "        start_finish_Z = 0.0\n",
    "    else:\n",
    "        start_finish_miles = None\n",
    "        start_finish_Z = None\n",
    "    if len(seq) < 2:\n",
    "        pass\n",
    "    else:\n",
    "        start_finish_miles = hop_distance(seq[0], seq[-1], geo_data, return_zero_dist=True)\n",
    "    if (len(hop_distances)==1) and (start_finish_miles>0.0): # start_finish_Z = NaN for single-hop books not ending at origin\n",
    "        start_finish_Z = None\n",
    "    elif len(hop_distances) > 1:\n",
    "        hop_std = np.std(hop_distances)\n",
    "        hop_mean = np.mean(hop_distances)\n",
    "        if hop_std > 0.01:\n",
    "            start_finish_Z = (start_finish_miles - hop_mean)/hop_std \n",
    "    return(total_distance, len(hop_distances), start_finish_miles, start_finish_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d525ac-83e6-496a-8e07-79812d195d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.5 s, sys: 7.54 ms, total: 24.5 s\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "distances = defaultdict(lambda: defaultdict(float))\n",
    "conlit_distances = conlit['gpe_sequences'].apply(sequence_distance, args=(geo, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bef9c-23e8-4283-9d98-fd22c9987c5a",
   "metadata": {},
   "source": [
    "### Save distances to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f08c90-d7c8-4ec6-8506-2e99f80aa29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conlit['dist_miles'], conlit['hops'], conlit['Start_Finish_Miles'], conlit['Start_Finish_Z'] = zip(*conlit_distances)\n",
    "conlit[['dist_miles', 'hops', 'Start_Finish_Miles', 'Start_Finish_Z']].to_csv(os.path.join(derived_dir, conlit_distances_output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5892a-7731-4ad1-b615-5fd406e9cb85",
   "metadata": {},
   "source": [
    "## EARLY GPE distances\n",
    "\n",
    "### Prepare early file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5860a096-f9c6-424a-b015-7b060931ecd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_hoplist(file_path, label='gpe'):\n",
    "    df = pd.read_json(\n",
    "        file_path,\n",
    "        lines=True\n",
    "    ).explode('chars')\n",
    "    \n",
    "    book_ids = []\n",
    "    char_ids = []\n",
    "    gpe_lists = []\n",
    "    seq_lists = []\n",
    "    for _, row in df.iterrows():\n",
    "        book_id = row.book_id\n",
    "        d = row.loc['chars']\n",
    "        char_id = d['char_id']\n",
    "        seq = d['sequence']\n",
    "        seq_list = [place_dict['place'] for place_dict in seq]\n",
    "        gpe_list = []\n",
    "        for place_dict in seq:\n",
    "            for i in range(place_dict['count']):\n",
    "                gpe_list.append(place_dict['place'])\n",
    "        seq_lists.append(seq_list)\n",
    "        gpe_lists.append(gpe_list)\n",
    "        book_ids.append(book_id)\n",
    "        char_ids.append(char_id)\n",
    "\n",
    "    result = pd.DataFrame(\n",
    "        {\n",
    "            'book_id':book_ids,\n",
    "            'char_id':char_ids,\n",
    "            f'{label}_places':gpe_lists,\n",
    "            f'{label}_sequences':seq_lists\n",
    "        }\n",
    "    )\n",
    "    return(result)\n",
    "\n",
    "# read hoplists\n",
    "early_gpes = read_hoplist(os.path.join(raw_dir, 'hoplist.gpe.all.jsonl.bz2'), label='gpe')\n",
    "\n",
    "# read base data\n",
    "base_early = pd.read_csv(\n",
    "    os.path.join(raw_dir, 'inf_gender.1.tsv.gz'),\n",
    "    sep='\\t',\n",
    ").drop(columns=['prob'])\n",
    "\n",
    "# restrict to protagonists\n",
    "early = base_early.merge(\n",
    "    early_gpes, how='left', on=['book_id', 'char_id']\n",
    ")\n",
    "\n",
    "# add token count\n",
    "early_token_counts = pd.read_csv(\n",
    "    os.path.join(raw_dir, 'book_lengths.tsv'),\n",
    "    sep='\\t',\n",
    "    skiprows=1,\n",
    "    names=['book_id', 'Tokens']\n",
    ")\n",
    "early = early.merge(early_token_counts, how='left', on=['book_id'])\n",
    "\n",
    "# add empty distance column\n",
    "early['dist_miles'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369353d4-2c65-409e-96cc-cf3a04f81c90",
   "metadata": {},
   "source": [
    "### Calculate distances\n",
    "\n",
    "Need to read geo data again so that we can restrict to GPEs that exist in early data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "777596ca-604a-46ff-a35d-1bec19fd1e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reread geo data to retain relevant locations, etc.\n",
    "geo = pd.read_csv(\n",
    "    os.path.join(geo_dir, 'geo.tsv.gz'),\n",
    "    sep='\\t',\n",
    "    low_memory=False,\n",
    ")\n",
    "geo = geo.loc[geo.lang=='en']\n",
    "geo.set_index('text_string', inplace=True)\n",
    "\n",
    "# hand review data\n",
    "hand = pd.read_csv(\n",
    "    os.path.join(geo_dir, 'handreview.tsv'),\n",
    "    sep='\\t',\n",
    "    index_col='text_string'\n",
    ")\n",
    "\n",
    "# restore some items from C19 hand review\n",
    "hand.loc[\n",
    "    [\n",
    "        'hollywood', \n",
    "        'dallas', \n",
    "        'florence', \n",
    "        'kingston',\n",
    "        'berkeley', \n",
    "        'queens', \n",
    "        'phoenix', \n",
    "        'woodstock', \n",
    "        'surrey',\n",
    "        'orlando'\n",
    "    ], \n",
    "    'ignore'\n",
    "] = 0\n",
    "\n",
    "# improve alises\n",
    "hand.loc['kingston', 'alias_to'] = 'kingston jamaica'\n",
    "hand.loc['baltic', ['ignore', 'alias_to']] = [0, 'baltic sea']\n",
    "\n",
    "# alias places\n",
    "for original_place, alias_to in hand.loc[(~hand.alias_to.isna()) & (~hand.ignore.equals(1)) & (hand.alias_to.isin(geo.index)), 'alias_to'].items():\n",
    "    geo.loc[original_place] = geo.loc[alias_to]\n",
    "\n",
    "# drop ignored places\n",
    "geo = geo.drop(hand.loc[hand.ignore==1].index, errors='ignore')\n",
    "\n",
    "# drop unused places\n",
    "used_gpes = Counter()\n",
    "for sequence in early.gpe_sequences:\n",
    "    used_gpes.update([regularize_string(i) for i in sequence])\n",
    "geo.drop(geo.loc[~geo.index.isin(used_gpes)].index, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "609a7f63-9781-4b60-baf1-49944a845140",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.3 s, sys: 28.7 ms, total: 41.3 s\n",
      "Wall time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "distances = defaultdict(lambda: defaultdict(float))\n",
    "early_distances = early['gpe_sequences'].apply(sequence_distance, args=(geo, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e60fc2-f6d5-47ba-b755-6981de7c80d9",
   "metadata": {},
   "source": [
    "### Save distances to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e73bbae-c375-4e7b-9850-b4f1ef7024b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early['dist_miles'], early['hops'], early['Start_Finish_Miles'], early['Start_Finish_Z'] = zip(*early_distances)\n",
    "early.set_index('book_id', inplace=True)\n",
    "early[['dist_miles', 'hops', 'Start_Finish_Miles', 'Start_Finish_Z']].to_csv(os.path.join(derived_dir, early_distances_output_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
